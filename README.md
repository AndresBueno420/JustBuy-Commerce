[![IaC Tool](https://img.shields.io/badge/IaC-CloudFormation-orange?logo=amazon-aws)](https://aws.amazon.com/cloudformation/)
[![Cloud Provider](https://img.shields.io/badge/Cloud-AWS-232F3E?logo=amazon-aws&logoColor=FF9900)](https://aws.amazon.com/)
[![Back-end](https://img.shields.io/badge/Backend-Node.js%20%26%20Express-448833?logo=nodedotjs)](https://nodejs.org/)
[![Database](https://img.shields.io/badge/Database-AWS%20RDS-527FFF?logo=amazon-aws)](https://aws.amazon.com/rds/)

```math
K(\mathbf{x}_i, \mathbf{x}_j) = \mathbf{x}_i^T \mathbf{x}_j
```
#Scalable E-commerce Deployment on AWS with IaC $\mu$, $\sigma$

## Project Overview

This project focuses on deploying a simulated **E-commerce Platform** on Amazon Web Services (**AWS**) using **Infrastructure as Code (IaC)** principles. We have leveraged **AWS CloudFormation** to build a secure, highly-available, and scalable three-tier architecture that can be deployed repeatedly and reliably.

### üèõÔ∏è Context & Course Details

| Field | Details |
| :--- | :--- |
| **Institution** | Universidad ICESI |
| **Course** | Plataformas III (Infrastructure III) |
| **Project Members** | Andres Bueno Cardona, Jean Paul Atkinson Vidales |
| **Primary Tool** | AWS CloudFormation |

---

## üéØ Project Goal

Our main goal was to deploy a simple e-commerce application (built with **Node.js/Express.js**) on AWS, ensuring it meets the following core requirements:

1.  **Scalability:** Implementation of an **Auto Scaling Group (ASG)** behind an **Application Load Balancer (ALB)** to handle traffic spikes.
2.  **Security:** Using a **Virtual Private Cloud (VPC)** with private subnets for application servers and the database, accessed securely via a **Bastion Host** and **Session Manager**.
3.  **Reproducibility:** Defining **all** infrastructure (Network, Compute, Database) through **CloudFormation templates**.

---

## Application Components

The deployed application includes the following basic functionalities:

* **Front-end:** HTML, CSS, and JavaScript (Client-side logic).
* **Back-end:** Node.js with Express.js, providing a **REST API**.
* **Database:** AWS **RDS** (Database Engine), ensuring persistent data storage.

---

## AWS Architecture Deployed

The infrastructure is defined in the `cloudformation/` folder and follows best practices for cloud deployments:

| Component | Purpose | AWS Service(s) |
| :--- | :--- | :--- |
| **Network** | Secure isolation and connectivity. | VPC, Subnets (Public/Private), Internet Gateway, NAT Gateway. |
| **Load Balancing** | Distributes incoming web traffic across multiple servers. | Application Load Balancer (ALB). |
| **Application Layer** | Runs the Node.js application. | EC2 Instances within an Auto Scaling Group (ASG). |
| **Database Layer** | Stores persistent application data. | AWS RDS (e.g., PostgreSQL/MySQL) in a Private Subnet. |
| **Monitoring** | Tracks performance and sends alerts. | CloudWatch (Metrics & Alarms) and SNS (Notifications). |
| **Secure Access** | Securely manages remote access to EC2. | Bastion Host, Session Manager. |



-----


1. **SVM (Support Vector Machine)**  
   - Implementado con `sklearn.svm.SVC` en un pipeline con `StandardScaler`.
   - El objetivo matem√°tico del SVM es encontrar un hiperplano √≥ptimo que maximice el margen (la distancia) entre los vectores de soporte de las diferentes clases en el espacio de       caracter√≠sticas.Justificaci√≥n de Hiperpar√°metros: Dado que los datos pueden no ser linealmente separables, se exploraron diferentes kernels :Lineal:
```math
K(\mathbf{x}_i, \mathbf{x}_j) = \mathbf{x}_i^T \mathbf{x}_j
```
   RBF (Base Radial):
```math
K(\mathbf{x}_i, \mathbf{x}_j) = \exp(-\gamma \|\mathbf{x}_i - \mathbf{x}_j\|^2)
```

   que mapea los datos a un espacio de mayor dimensi√≥n.El hiperpar√°metro  $C$ 7 (par√°metro de regularizaci√≥n) se ajust√≥ para controlar la penalizaci√≥n por clasificaci√≥n incorrecta, gestionando as√≠ el balance entre un margen amplio y la minimizaci√≥n de los errores de entrenamiento.
   - Ajuste de hiperpar√°metros mediante `GridSearchCV` con validaci√≥n cruzada (k=3).  
   - Par√°metros explorados: `C`, `kernel` (`linear`, `rbf`), `gamma`.  

2. **Random Forest**  
   - Modelo de ensamblado de √°rboles (`sklearn.ensemble.RandomForestClassifier`).
   - El modelo construye m√∫ltiples √°rboles de decisi√≥n (n_estimators) 9 sobre subconjuntos aleatorios de las caracter√≠sticas. La decisi√≥n de divisi√≥n en cada nodo se optimiza minimizando una m√©trica de impureza, en este caso, el √çndice de Gini:
```math
Gini = 1 - \sum_{i=1}^{C} (p_i)^2
```
   donde $p_i$ es la probabilidad de que una muestra en el nodo pertenezca a la clase $i$. Se exploraron tambi√©n max_depth y min_samples_split para controlar la profundidad de los √°rboles y prevenir el sobreajuste.
   - Ajuste con rejilla de hiperpar√°metros (`n_estimators`, `max_depth`, `min_samples_split`, `min_samples_leaf`).  
   - Permite extraer **importancia de caracter√≠sticas**, √∫til para interpretaci√≥n.

3. **XGBoost**  
   - Clasificador de gradiente optimizado (`xgboost.XGBClassifier`).
   - A diferencia del RF, XGBoost construye √°rboles de forma secuencial. Cada nuevo √°rbol se entrena para corregir los errores residuales del modelo anterior, optimizando iterativamente el gradiente de una funci√≥n de p√©rdida (como la log-loss para clasificaci√≥n multiclase). 
   - Par√°metros: `max_depth`, `learning_rate`, `subsample`, `colsample_bytree`.  
   - Entrenado con validaci√≥n cruzada id√©ntica (cv=3, f1-weighted).  

El proceso de entrenamiento se centraliza en `src/models/train_models.py`, 
donde se definen los **grids**, la divisi√≥n de datos (80 % train / 20 % test), 
y se exportan los artefactos entrenados (`.joblib`) junto con los reportes JSON 
y las matrices de confusi√≥n.

### 2.4. M√©trica de Evaluaci√≥n
Como m√©trica de evaluaci√≥n principal para la optimizaci√≥n (GridSearchCV) y la comparaci√≥n final, se seleccion√≥ el F1-Macro Score131313.Justificaci√≥n Matem√°tica: El F1-Score es la media arm√≥nica de la Precisi√≥n y el Recall:
```math 
F_1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}
```
Esta m√©trica es ideal para datasets con desbalance de clases, como el actual (donde clases como caminar son m√°s frecuentes que girar). Al utilizar el promedio 'macro'14, se calcula el F1-Score para cada clase de forma independiente y luego se promedia, asegurando que el rendimiento en clases minoritarias tenga el mismo peso en la evaluaci√≥n final que el de las clases mayoritarias.

